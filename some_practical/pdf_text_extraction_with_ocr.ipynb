{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "doc_path = os.getenv('pdf_coop')\n",
    "pages = convert_from_path(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def deskew(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bitwise_not(gray)\n",
    "    coords = np.column_stack(np.where(gray > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    \n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "def extract_text_from_image(image):\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_texts = []\n",
    "\n",
    "for page in pages:\n",
    "    # Step 2: Preprocess the image (deskew)\n",
    "    preprocessed_image = deskew(np.array(page))\n",
    "\n",
    "    # Step 3: Extract text using OCR\n",
    "    text = extract_text_from_image(preprocessed_image)\n",
    "    extracted_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GAMME\\n'\n",
      " '\\n'\n",
      " 'JUNIOR SAVINGS ACCOUNT\\n'\n",
      " 'PRODUCT OVERVIEW:\\n'\n",
      " '\\n'\n",
      " 'This is specifically crafted for children aged\\n'\n",
      " \"0-14 years. It is established in the child's name\\n\"\n",
      " 'but is typically managed by parents or guard-\\n'\n",
      " 'ians until the child reaches an age where\\n'\n",
      " 'he/she can independently oversee their\\n'\n",
      " 'finances.\\n'\n",
      " '\\n'\n",
      " 'Key Features\\n'\n",
      " '\\n'\n",
      " 'e\\n'\n",
      " '\\n'\n",
      " 'The account is registered in the\\n'\n",
      " \"child's name, setting the founda-\\n\"\n",
      " 'tion for their financial future.\\n'\n",
      " 'Bears an interest rate of 7.175%,\\n'\n",
      " 'fostering the growth of their sav-\\n'\n",
      " 'ings.\\n'\n",
      " '\\n'\n",
      " 'Facilitates financial independence\\n'\n",
      " 'by enabling withdrawals when the\\n'\n",
      " 'junior accountholder reaches the\\n'\n",
      " 'age of youth.\\n'\n",
      " '\\n'\n",
      " 'Children demonstrating indepen-\\n'\n",
      " 'dent income, who initiate their\\n'\n",
      " 'account, enjoy unrestricted access\\n'\n",
      " 'to their deposits.\\n'\n",
      " '\\n'\n",
      " 'Receive a Birr 100 credit incentive\\n'\n",
      " 'when the average six-month\\n'\n",
      " 'deposit level reaches Birr 30,000\\n'\n",
      " 'and above.\\n'\n",
      " '\\n'\n",
      " 'BENEFITS\\n'\n",
      " '\\n'\n",
      " 'Establish a financial nest egg for\\n'\n",
      " 'children as they approach adult-\\n'\n",
      " 'hood.\\n'\n",
      " '\\n'\n",
      " 'Instill a savings culture from an\\n'\n",
      " 'early age, empowering parents to\\n'\n",
      " 'teach valuable financial lessons.\\n'\n",
      " 'Convenience of fee-free transac-\\n'\n",
      " 'tions, making banking interactions\\n'\n",
      " 'hassle-free.\\n'\n",
      " '\\n'\n",
      " 'Unlock additional incentives pro-\\n'\n",
      " 'vided by the bank, subject to speci-\\n'\n",
      " 'fied criteria.\\n'\n",
      " '\\n'\n",
      " 'ELIGIBLE CANDIDATES:\\n'\n",
      " '\\n'\n",
      " 'This account caters to newborns,\\n'\n",
      " 'infants, children under the age of\\n'\n",
      " 'fifteen, and those who engage in\\n'\n",
      " 'work up to the age of fifteen.\\n'\n",
      " '\\n'\n",
      " 'ay\\n'\n",
      " '\\n'\n",
      " 'TARGET\\n'\n",
      " 'CUSTOMERS\\n'\n",
      " '\\n'\n",
      " '& Newborn at Hospital\\n'\n",
      " '\\n'\n",
      " '& Student at school and Kinder-\\n'\n",
      " 'garten\\n'\n",
      " '\\n'\n",
      " '& NGOs that work with kids\\n'\n",
      " '\\n'\n",
      " 'CROSS SELLING\\n'\n",
      " 'eto PRODUCTS\\n'\n",
      " '\\n'\n",
      " 'WW * Gudunfa Gamme Saving\\n'\n",
      " 'account.\\n'\n",
      " '\\n'\n",
      " '* When they are of age, Darga-\\n'\n",
      " '\\n'\n",
      " 'go saving account.\\n'\n",
      " '\\n'\n",
      " 'REMARKS\\n'\n",
      " '\\n'\n",
      " 'It is a unique opportunity to pave the\\n'\n",
      " \"way for a child's financial success.\\n\"\n",
      " '\\n'\n",
      " 'PRODUCT CATALOG\\n'\n",
      " '\\n'\n",
      " 'PAGE 04\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(extracted_texts[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Step 1: Remove hyphenated line breaks (e.g., 'guard-\\n ians' -> 'guardians')\n",
    "    cleaned_text = re.sub(r'-\\n\\s*', '', text)\n",
    "    \n",
    "    # Step 2: Replace single newlines between lines with spaces\n",
    "    cleaned_text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', cleaned_text)\n",
    "    match = re.search(r'PAGE (\\d+)', text)\n",
    "    \n",
    "    # Step 3: Remove \"PRODUCT CATALOG\" and everything after\n",
    "    cleaned_text = re.sub(r'PRODUCT CATALOG.*$', '', cleaned_text, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Step 4: Capture the page number and split the text at \"PAGE xx\"\n",
    "\n",
    "    \n",
    "    if match:\n",
    "        # print(\"match startes at: \",match.start())\n",
    "        # Split into content and page sections\n",
    "        content = cleaned_text[:match.start()].strip()\n",
    "        page = match.group()\n",
    "        # match_obj.group()\n",
    "    else:\n",
    "        content = cleaned_text\n",
    "        page = None  # No page section found\n",
    "    \n",
    "    return content\n",
    "# cleaned = clean_text(extracted_text[5])\n",
    "full_text = []\n",
    "for extracted_text in extracted_texts:\n",
    "    full_text.append(clean_text(extracted_text))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "\n",
    "embeddings =  NomicEmbeddings(model=\"nomic-embed-text-v1.5\",)\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\" \")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleaned_content:  \u001b[38;5;66;03m# Check if there's valid content to embed\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Step 4: Get the embedding for the cleaned content\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     content_embedding \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39membed_query(cleaned_content)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontent_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add the embedding to the FAISS index\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     vector_store\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39madd(docstore_id, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: cleaned_content})\n\u001b[1;32m     11\u001b[0m     vector_store\u001b[38;5;241m.\u001b[39mindex_to_docstore_id[docstore_id] \u001b[38;5;241m=\u001b[39m docstore_id\n",
      "File \u001b[0;32m~/AI-Agent/RAG_Project/venv/lib/python3.11/site-packages/faiss/class_wrappers.py:227\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplacement_add\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Adds vectors to the index.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    The index must be trained before vectors can be added to it.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    The vectors are implicitly numbered in sequence. When `n` vectors are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m        `dtype` must be float32.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     n, d \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[1;32m    229\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "docstore_id = 0  # Start with the first document ID\n",
    "\n",
    "for extracted_text in extracted_texts:  # Loop through your list of texts\n",
    "    cleaned_content = clean_text(extracted_text)  # Clean the text\n",
    "    \n",
    "    if cleaned_content:  # Check if there's valid content to embed\n",
    "        # Step 4: Get the embedding for the cleaned content\n",
    "        content_embedding = embeddings.embed_query(cleaned_content)\n",
    "        vector_store.index.add([content_embedding])  # Add the embedding to the FAISS index\n",
    "        vector_store.docstore.add(docstore_id, {\"content\": cleaned_content})\n",
    "        vector_store.index_to_docstore_id[docstore_id] = docstore_id\n",
    "        docstore_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: content, Value: This is the content.\n",
      "Key: page, Value: 04\n"
     ]
    }
   ],
   "source": [
    "# Example dictionary\n",
    "example_dict = {\n",
    "    'content': \"This is the content.\",\n",
    "    'page': '04'\n",
    "}\n",
    "\n",
    "# Loop through the dictionary to get keys and values\n",
    "for key, value in example_dict.items():\n",
    "    print(f\"Key: {key}, Value: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No page number found\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_page_match(text):\n",
    "    # Define the regular expression pattern to find \"PAGE xx\"\n",
    "    page_match = re.search(r'PAGE \\d+', text)\n",
    "    \n",
    "    if page_match:\n",
    "        print(page_match)  # This will output the match object\n",
    "        return page_match\n",
    "    else:\n",
    "        print(\"No page number found\")\n",
    "        return None\n",
    "\n",
    "# cleaned_text = re.sub(r'PRODUCT CATALOG.*$', '', cleaned_text, flags=re.DOTALL).strip()\n",
    "\n",
    "# Find the page match\n",
    "match_obj = find_page_match(extracted_text[5])\n",
    "\n",
    "# If needed, access properties of the match object\n",
    "if match_obj:\n",
    "    print(\"Span:\", match_obj.span())  # Output: (1663, 1670)\n",
    "    print(\"Start:\", match_obj.start())\n",
    "    print(\"Matched text:\", match_obj.group())  # Output: 'PAGE 04'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits = text_splitter.split_text(extracted_text)\n",
    "# splits[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
