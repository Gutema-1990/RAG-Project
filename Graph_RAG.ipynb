{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Tuple, Dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "import heapq\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from spacy.cli import download\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "from helper_functions import *\n",
    "# from evalute_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DocumentProcessor class\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DocumentProcessor with a text splitter and OpenAI embeddings.\n",
    "        \n",
    "        Attributes:\n",
    "        - text_splitter: An instance of RecursiveCharacterTextSplitter with specified chunk size and overlap.\n",
    "        - embeddings: An instance of OpenAIEmbeddings used for embedding documents.\n",
    "        \"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Processes a list of documents by splitting them into smaller chunks and creating a vector store.\n",
    "        \n",
    "        Args:\n",
    "        - documents (list of str): A list of documents to be processed.\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: A tuple containing:\n",
    "          - splits (list of str): The list of split document chunks.\n",
    "          - vector_store (FAISS): A FAISS vector store created from the split document chunks and their embeddings.\n",
    "        \"\"\"\n",
    "        splits = self.text_splitter.split_documents(documents)\n",
    "        vector_store = FAISS.from_documents(splits, self.embeddings)\n",
    "        return splits, vector_store\n",
    "\n",
    "    def create_embeddings_batch(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Creates embeddings for a list of texts in batches.\n",
    "        \n",
    "        Args:\n",
    "        - texts (list of str): A list of texts to be embedded.\n",
    "        - batch_size (int, optional): The number of texts to process in each batch. Default is 32.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: An array of embeddings for the input texts.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embeddings.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def compute_similarity_matrix(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes a cosine similarity matrix for a given set of embeddings.\n",
    "        \n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: A cosine similarity matrix for the input embeddings.\n",
    "        \"\"\"\n",
    "        return cosine_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Concepts class\n",
    "class Concepts(BaseModel):\n",
    "    concepts_list: List[str] = Field(description=\"List of concepts\")\n",
    "\n",
    "# Define the KnowledgeGraph class\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the KnowledgeGraph with a graph, lemmatizer, and NLP model.\n",
    "        \n",
    "        Attributes:\n",
    "        - graph: An instance of a networkx Graph.\n",
    "        - lemmatizer: An instance of WordNetLemmatizer.\n",
    "        - concept_cache: A dictionary to cache extracted concepts.\n",
    "        - nlp: An instance of a spaCy NLP model.\n",
    "        - edges_threshold: A float value that sets the threshold for adding edges based on similarity.\n",
    "        \"\"\"\n",
    "        self.graph = nx.Graph()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.concept_cache = {}\n",
    "        self.nlp = self._load_spacy_model()\n",
    "        self.edges_threshold = 0.8\n",
    "\n",
    "    def build_graph(self, splits, llm, embedding_model):\n",
    "        \"\"\"\n",
    "        Builds the knowledge graph by adding nodes, creating embeddings, extracting concepts, and adding edges.\n",
    "        \n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - llm: An instance of a large language model.\n",
    "        - embedding_model: An instance of an embedding model.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self._add_nodes(splits)\n",
    "        embeddings = self._create_embeddings(splits, embedding_model)\n",
    "        self._extract_concepts(splits, llm)\n",
    "        self._add_edges(embeddings)\n",
    "\n",
    "    def _add_nodes(self, splits):\n",
    "        \"\"\"\n",
    "        Adds nodes to the graph from the document splits.\n",
    "        WordNetLemmatizer\n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        for i, split in enumerate(splits):\n",
    "            self.graph.add_node(i, content=split.page_content)\n",
    "\n",
    "    def _create_embeddings(self, splits, embedding_model):\n",
    "        \"\"\"\n",
    "        Creates embeddings for the document splits using the embedding model.\n",
    "        \n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - embedding_model: An instance of an embedding model.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: An array of embeddings for the document splits.\n",
    "        \"\"\"\n",
    "        texts = [split.page_content for split in splits]\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    def _compute_similarities(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity matrix for the embeddings.\n",
    "        \n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: A cosine similarity matrix for the embeddings.\n",
    "        \"\"\"\n",
    "        return cosine_similarity(embeddings)\n",
    "\n",
    "    def _load_spacy_model(self):\n",
    "        \"\"\"\n",
    "        Loads the spaCy NLP model, downloading it if necessary.\n",
    "        \n",
    "        Args:\n",
    "        - None\n",
    "        \n",
    "        Returns:\n",
    "        - spacy.Language: An instance of a spaCy NLP model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Downloading spaCy model...\")\n",
    "            download(\"en_core_web_sm\")\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def _extract_concepts_and_entities(self, content, llm):\n",
    "        \"\"\"\n",
    "        Extracts concepts and named entities from the content using spaCy and a large language model.\n",
    "        \n",
    "        Args:\n",
    "        - content (str): The content from which to extract concepts and entities.\n",
    "        - llm: An instance of a large language model.\n",
    "        \n",
    "        Returns:\n",
    "        - list: A list of extracted concepts and entities.\n",
    "        \"\"\"\n",
    "        if content in self.concept_cache:\n",
    "            return self.concept_cache[content]\n",
    "        \n",
    "        # Extract named entities using spaCy\n",
    "        doc = self.nlp(content)\n",
    "        named_entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"WORK_OF_ART\"]]\n",
    "        \n",
    "        # Extract general concepts using LLM\n",
    "        concept_extraction_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"Extract key concepts (excluding named entities) from the following text:\\n\\n{text}\\n\\nKey concepts:\"\n",
    "        )\n",
    "        concept_chain = concept_extraction_prompt | llm.with_structured_output(Concepts)\n",
    "        general_concepts = concept_chain.invoke({\"text\": content}).concepts_list\n",
    "        \n",
    "        # Combine named entities and general concepts\n",
    "        all_concepts = list(set(named_entities + general_concepts))\n",
    "        \n",
    "        self.concept_cache[content] = all_concepts\n",
    "        return all_concepts\n",
    "\n",
    "    def _extract_concepts(self, splits, llm):\n",
    "        \"\"\"\n",
    "        Extracts concepts for all document splits using multi-threading.\n",
    "        \n",
    "        Args:\n",
    "        - splits (list): A list of document splits.\n",
    "        - llm: An instance of a large language model.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            future_to_node = {executor.submit(self._extract_concepts_and_entities, split.page_content, llm): i \n",
    "                              for i, split in enumerate(splits)}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_node), total=len(splits), desc=\"Extracting concepts and entities\"):\n",
    "                node = future_to_node[future]\n",
    "                concepts = future.result()\n",
    "                self.graph.nodes[node]['concepts'] = concepts\n",
    "\n",
    "    def _add_edges(self, embeddings):\n",
    "        \"\"\"\n",
    "        Adds edges to the graph based on the similarity of embeddings and shared concepts.\n",
    "        \n",
    "        Args:\n",
    "        - embeddings (numpy.ndarray): An array of embeddings for the document splits.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        similarity_matrix = self._compute_similarities(embeddings)\n",
    "        num_nodes = len(self.graph.nodes)\n",
    "        \n",
    "        for node1 in tqdm(range(num_nodes), desc=\"Adding edges\"):\n",
    "            for node2 in range(node1 + 1, num_nodes):\n",
    "                similarity_score = similarity_matrix[node1][node2]\n",
    "                if similarity_score > self.edges_threshold:\n",
    "                    shared_concepts = set(self.graph.nodes[node1]['concepts']) & set(self.graph.nodes[node2]['concepts'])\n",
    "                    edge_weight = self._calculate_edge_weight(node1, node2, similarity_score, shared_concepts)\n",
    "                    self.graph.add_edge(node1, node2, weight=edge_weight, \n",
    "                                        similarity=similarity_score,\n",
    "                                        shared_concepts=list(shared_concepts))\n",
    "\n",
    "    def _calculate_edge_weight(self, node1, node2, similarity_score, shared_concepts, alpha=0.7, beta=0.3):\n",
    "        \"\"\"\n",
    "        Calculates the weight of an edge based on similarity score and shared concepts.\n",
    "        \n",
    "        Args:\n",
    "        - node1 (int): The first node.\n",
    "        - node2 (int): The second node.\n",
    "        - similarity_score (float): The similarity score between the nodes.\n",
    "        - shared_concepts (set): The set of shared concepts between the nodes.\n",
    "        - alpha (float, optional): The weight of the similarity score. Default is 0.7.\n",
    "        - beta (float, optional): The weight of the shared concepts. Default is 0.3.\n",
    "        \n",
    "        Returns:\n",
    "        - float: The calculated weight of the edge.\n",
    "        \"\"\"\n",
    "        max_possible_shared = min(len(self.graph.nodes[node1]['concepts']), len(self.graph.nodes[node2]['concepts']))\n",
    "        normalized_shared_concepts = len(shared_concepts) / max_possible_shared if max_possible_shared > 0 else 0\n",
    "        return alpha * similarity_score + beta * normalized_shared_concepts\n",
    "\n",
    "    def _lemmatize_concept(self, concept):\n",
    "        \"\"\"\n",
    "        Lemmatizes a given concept.\n",
    "        \n",
    "        Args:\n",
    "        - concept (str): The concept to be lemmatized.\n",
    "        \n",
    "        Returns:\n",
    "        - str: The lemmatized concept.\n",
    "        \"\"\"\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in concept.lower().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Define the Visualizer class\n",
    "class Visualizer:\n",
    "    @staticmethod\n",
    "    def visualize_traversal(graph, traversal_path):\n",
    "        \"\"\"\n",
    "        Visualizes the traversal path on the knowledge graph with nodes, edges, and traversal path highlighted.\n",
    "\n",
    "        Args:\n",
    "        - graph (networkx.Graph): The knowledge graph containing nodes and edges.\n",
    "        - traversal_path (list of int): The list of node indices representing the traversal path.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        traversal_graph = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes and edges from the original graph\n",
    "        for node in graph.nodes():\n",
    "            traversal_graph.add_node(node)\n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            traversal_graph.add_edge(u, v, **data)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        \n",
    "        # Generate positions for all nodes\n",
    "        pos = nx.spring_layout(traversal_graph, k=1, iterations=50)\n",
    "        \n",
    "        # Draw regular edges with color based on weight\n",
    "        edges = traversal_graph.edges()\n",
    "        edge_weights = [traversal_graph[u][v].get('weight', 0.5) for u, v in edges]\n",
    "        nx.draw_networkx_edges(traversal_graph, pos, \n",
    "                               edgelist=edges,\n",
    "                               edge_color=edge_weights,\n",
    "                               edge_cmap=plt.cm.Blues,\n",
    "                               width=2,\n",
    "                               ax=ax)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
    "                               node_color='lightblue',\n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "        \n",
    "        # Draw traversal path with curved arrows\n",
    "        edge_offset = 0.1\n",
    "        for i in range(len(traversal_path) - 1):\n",
    "            start = traversal_path[i]\n",
    "            end = traversal_path[i + 1]\n",
    "            start_pos = pos[start]\n",
    "            end_pos = pos[end]\n",
    "            \n",
    "            # Calculate control point for curve\n",
    "            mid_point = ((start_pos[0] + end_pos[0]) / 2, (start_pos[1] + end_pos[1]) / 2)\n",
    "            control_point = (mid_point[0] + edge_offset, mid_point[1] + edge_offset)\n",
    "            \n",
    "            # Draw curved arrow\n",
    "            arrow = patches.FancyArrowPatch(start_pos, end_pos,\n",
    "                                            connectionstyle=f\"arc3,rad={0.3}\",\n",
    "                                            color='red',\n",
    "                                            arrowstyle=\"->\",\n",
    "                                            mutation_scale=20,\n",
    "                                            linestyle='--',\n",
    "                                            linewidth=2,\n",
    "                                            zorder=4)\n",
    "            ax.add_patch(arrow)\n",
    "        \n",
    "        # Prepare labels for the nodes\n",
    "        labels = {}\n",
    "        for i, node in enumerate(traversal_path):\n",
    "            concepts = graph.nodes[node].get('concepts', [])\n",
    "            label = f\"{i + 1}. {concepts[0] if concepts else ''}\"\n",
    "            labels[node] = label\n",
    "        \n",
    "        for node in traversal_graph.nodes():\n",
    "            if node not in labels:\n",
    "                concepts = graph.nodes[node].get('concepts', [])\n",
    "                labels[node] = concepts[0] if concepts else ''\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(traversal_graph, pos, labels, font_size=8, font_weight=\"bold\", ax=ax)\n",
    "        \n",
    "        # Highlight start and end nodes\n",
    "        start_node = traversal_path[0]\n",
    "        end_node = traversal_path[-1]\n",
    "        \n",
    "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
    "                               nodelist=[start_node], \n",
    "                               node_color='lightgreen', \n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "        \n",
    "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
    "                               nodelist=[end_node], \n",
    "                               node_color='lightcoral', \n",
    "                               node_size=3000,\n",
    "                               ax=ax)\n",
    "        \n",
    "        ax.set_title(\"Graph Traversal Flow\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add colorbar for edge weights\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=min(edge_weights), vmax=max(edge_weights)))\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Edge Weight', rotation=270, labelpad=15)\n",
    "        \n",
    "        # Add legend\n",
    "        regular_line = plt.Line2D([0], [0], color='blue', linewidth=2, label='Regular Edge')\n",
    "        traversal_line = plt.Line2D([0], [0], color='red', linewidth=2, linestyle='--', label='Traversal Path')\n",
    "        start_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=15, label='Start Node')\n",
    "        end_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=15, label='End Node')\n",
    "        legend = plt.legend(handles=[regular_line, traversal_line, start_point, end_point], loc='upper left', bbox_to_anchor=(0, 1), ncol=2)\n",
    "        legend.get_frame().set_alpha(0.8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def print_filtered_content(traversal_path, filtered_content):\n",
    "        \"\"\"\n",
    "        Prints the filtered content of visited nodes in the order of traversal.\n",
    "\n",
    "        Args:\n",
    "        - traversal_path (list of int): The list of node indices representing the traversal path.\n",
    "        - filtered_content (dict of int: str): A dictionary mapping node indices to their filtered content.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        print(\"\\nFiltered content of visited nodes in order of traversal:\")\n",
    "        for i, node in enumerate(traversal_path):\n",
    "            print(f\"\\nStep {i + 1} - Node {node}:\")\n",
    "            print(f\"Filtered Content: {filtered_content.get(node, 'No filtered content available')[:200]}...\")  # Print first 200 characters\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAG:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the GraphRAG system with components for document processing, knowledge graph construction,\n",
    "        querying, and visualization.\n",
    "        \n",
    "        Attributes:\n",
    "        - llm: An instance of a large language model (LLM) for generating responses.\n",
    "        - embedding_model: An instance of an embedding model for document embeddings.\n",
    "        - document_processor: An instance of the DocumentProcessor class for processing documents.\n",
    "        - knowledge_graph: An instance of the KnowledgeGraph class for building and managing the knowledge graph.\n",
    "        - query_engine: An instance of the QueryEngine class for handling queries (initialized as None).\n",
    "        - visualizer: An instance of the Visualizer class for visualizing the knowledge graph traversal.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "        self.embedding_model = OpenAIEmbeddings()\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.knowledge_graph = KnowledgeGraph()\n",
    "        self.query_engine = None\n",
    "        self.visualizer = Visualizer()\n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Processes a list of documents by splitting them into chunks, embedding them, and building a knowledge graph.\n",
    "        \n",
    "        Args:\n",
    "        - documents (list of str): A list of documents to be processed.\n",
    "        \n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        splits, vector_store = self.document_processor.process_documents(documents)\n",
    "        self.knowledge_graph.build_graph(splits, self.llm, self.embedding_model)\n",
    "        self.query_engine = QueryEngine(vector_store, self.knowledge_graph, self.llm)\n",
    "\n",
    "    def query(self, query: str):\n",
    "        \"\"\"\n",
    "        Handles a query by retrieving relevant information from the knowledge graph and visualizing the traversal path.\n",
    "        \n",
    "        Args:\n",
    "        - query (str): The query to be answered.\n",
    "        \n",
    "        Returns:\n",
    "        - str: The response to the query.\n",
    "        \"\"\"\n",
    "        response, traversal_path, filtered_content = self.query_engine.query(query)\n",
    "        \n",
    "        if traversal_path:\n",
    "            self.visualizer.visualize_traversal(self.knowledge_graph.graph, traversal_path)\n",
    "        else:\n",
    "            print(\"No traversal path to visualize.\")\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"/home/name-1/AI-Agent/RAG_Project/RAG_Project/data/Understanding_Climate_Change.pdf\"\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()\n",
    "documents = documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag = GraphRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag.process_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the main cause of climate change?\"\n",
    "response = graph_rag.query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
